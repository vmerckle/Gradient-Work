\section{direct result}

Minimization of a linear combination of ('neurons' $\{\phi(\theta)\}_{\theta \in \Theta}$) through an unknown measure $\mu$: $J^* = \min_{\mu \in \mathcal{M}(\Theta)} J(\mu) = R(\int \phi \dd \mu) + G(\mu)$

With $R$ a convex loss function and $G$ a convex regularizer, $\mathcal{M}(\Theta)$ the set of signed measures on the parameter space $\Theta$.

Discretize the measure into $m$ particles: $\min_{w \in \R^m \\\ \theta \in \Theta^m} J_m(w, \theta) = J( \frac{1}{m} \sum_{i=1}^{m} w_i \delta_{\theta_i})$

Proved: if WGF cvg, it cvg to global minimizer. If $(w^{(m)}(t), \theta^{(m)}(t))_{t>0}$ are gradient flows for $J_m$ then, with the corresponding measure $\mu_{m,t} = \frac{1}{m} \sum_{i=1}^{m} w^{(m)}_i(t) \sigma_{\theta_i^{(m)}(t)}$ (a WGF), $J(\mu_{m, t})$ cvg (with $m, t \rightarrow \inf$) to global minimizer of $J$.

p-Wasserstein distance between two measures $\mu, \nu \in \mathcal{P}(\R^d)$: $W_p(\mu, \nu)^p = \min_{\gamma \in \Pi(\mu, \nu)} \int |y - x|^p \dd \gamma(x, y)$

\section{simple result}

Cast the parameters $w \in \mathcal{W}^{d+1}$ into $r \in \R, \eta \in \mathcal{S}^d = {w \in \R^{d+1}, \norm{w}_2 = 1}$ with $w = r ~ \eta$. The two flows will have exactly the same dynamics.

Then measure $\nu= \frac{1}{m}\sum_{j=1}^{m} r^2_j \sigma_{\nu}$ satisfy this PDE : $\partial_t \nu_t(\eta) = -4 J(\eta | \nu_t) + \diver(\nu_t(\eta) \nabla J(\eta | \nu_t)) $

Prediction functions : $h = \frac{1}{m} \sum_{j=1}^{m} \psi(w_j) = \frac{1}{m}\sum_{j=1}^{m} r_j^2 \psi(\nu_j)$

Theorem 1: if by taking $m \leftarrow \inf$ at $t=0$, $\nu$ converges to $\nu_0$, then for any $t$, $\nu_{m, t}$ cvg to the unique WGF $\nu_t$.

Theorem 2: Take a $\nu_0$ with a support that includes all directions at initialization, if WGF $\nu_t$ cvg, it's to a global optimum.

\section{Math introduction}

\subsection{Gradient Flow}

Given a smooth function $a \rightarrow F(a)$, the gradient flow is gradient descent algorithm

$a^{l+1} = a^l - \gamma \nabla F(a^l)$

with a small enough $\gamma$. If $F$ is not smooth, the gradient flow is the proximal-point algorithm

$a^{l+1} = \prox^{||\cdot ||}_{\gamma F}(a^{(l)} = \argmin_a \frac{1}{2} \norm{ a - a^{(l)}}^2 + \gamma F(a)$

with a small enough $\gamma$.

If $F$ is defined on histograms, it makes sense to use the wasserstein distance $W^p$

\section{Gradient Flow vs Wasserstein GF}

Take a two layer ReLU network with $m$ neurons. Each neuron has a trainable parameter $w_i \in \R^d$ and and a fixed output sign $\alpha_i \in \{-1, 1\}$. Each of the $n$ data points of dimension $d-1$ are augmented with a $1$, so each sample is of dimension $n$. Each sample $x_j$ is stored as a row $X \in \R^{n \times d}$  and is associated with a scalar label $y_j \in \R$.

The output of one neuron is: $x \in \R^d \rightarrow \max(0, \inp{w_i}{x}) \alpha_i$, shorthand $\inp{w_i}{x}_+ ~ \alpha_i$.

The output of a network of $m$ neurons on one data point is $f(x) = \sum_{i=1}^{m} \max(0, \inp{w_i}{x}) ~ \alpha_i$

We store the neuron trainable parameter $w_i$ as the columns of $W \in \R^{d \times m}$. The loss for $n$ data points

\begin{equation}
	F(W) = \frac{1}{n} \sum_{j=1}^{n} \left(f(x_j) - y_j\right)^2
\end{equation}

Discretized, unregularized gradient descent with $\lambda \in \R^+$ stepsize: (resolving the non differentiable points $\max(0, 0)$ with 0 as gradient)

\begin{equation}
	W^{t+1} = W^t - \lambda \nabla F(W^t)
\end{equation}

Taking $\lambda \rightarrow 0$, we get the gradient flow.

Explicit discret gradient:

\begin{equation}
	\frac{\partial F}{\partial \vw^t_i} = \frac{\alpha^t_i}{n} \sum_{j=1}^{n} e^t_j ~ s^t_{i, j} ~ \vx_j
\end{equation}

With real $e^t_j = f(W^t) - y_j$ the "signed error on input $j$"

With boolean $s^t_{i, j} = \mathds{1}_{\inp{\vw^t_i}{\vx_j} > 0}$ the "is neuron $i$ activating on datapoint $j$. The vector $\vs^t_{i} \in \{0, 1\}^n$ would be the activation pattern of neuron $i$ at time $t$.

Remark: the gradient of a neuron is a linear combination of the data points it activates.

Discretized wasserstein prox step:

\begin{equation}
	W^{t+1} = \argmin_{W \in \R^{d \times m}} F(W) + \frac{1}{2 \step} W_p(W; W^t))
\end{equation}


EMD(POT library), d=squareeuclidian

\begin{align}
	W_2^2(W; W^t) &= \min_{\gamma} \inp{\gamma}{M}_F \\
	\text{s.t.  } \gamma ~ \mathbf{1} &= W \\
	\gamma^\top ~ \mathbf{1} &= W^t \\
	\gamma &\geq 0 \\
	M_{i, j} &= \lVert w_i - w_j \rVert^2_2
\end{align}

\section{Simple example 2D setting, grid jko}

From one dimensional data, we add a dimension filled with ones to act as a bias for the first layer. The output of one ReLU neuron for one data point $(x, 1) \in \R^2$:

\begin{equation}
	w, b, \alpha \in \R \rightarrow \max(0, w x + b) \alpha
\end{equation}

The loss against labels $y_j \in \R$ using squared loss of the whole network of neurons is the double sum:

\begin{equation}
	 \mathcal{L} = \sum_{j=1}^n \left(\left(\sum_{i=1}^m \max(0, w_i x_j + b_i) \alpha_i \right)  - y_j \right)^2
\end{equation}

The mean-field limit of this network requires taking an infinite-width ReLU network where parameters are described by a measure $\mu$, and its output by an integral:

\begin{equation}
	\int_{\R^2} m((w, b); x) ~ \dd \mu((w, b))
\end{equation}
 
To simplify things, we restrict $\alpha_i$ to $\{-1, 1\}$ and to not be a trainable parameter anymore. We keep the same expressivity(as long as we provide both a positive($\alpha_i=1$) and negative($\alpha_i=-1$) version of the neuron) but this change will slightly alter the training dynamic in some cases. For example , we can match the output of one neuron (of the original network) by simply scaling the first layer by the seconder layer ($\alpha$):

\begin{equation}
	\max(0, w_i x + b_i) \alpha_i = \max\left(0, |\alpha_i|(w_i x + b_i)\right) \ \text{sign}(\alpha_i)
\end{equation}

Our network with restricted $\alpha_i$ would describe this neuron using only two trainable parameters: $(|alpha_i| w_i, |\alpha_i| b_i)$ and fix its sign in the output.

The measure is on the parameter space. In order to do simulations we discretize the parameter space, by taking a uniform grid in $\R^2$ centered on $(0, 0)$: $(w_i, b_i)_{i=1, \dots m}$

We can see that we have the same output and expressivity as the regular ReLU network by taking a measure $\mu = \sum_{i=1}^{m} p_i \dirac_{\param_i = \vw_i}$ with ($\sum_i p_i=1$) and $m((w_i, b_i); x) = \max(0, w_i x + b_i) \alpha_i$, we have this equality:

\begin{equation}
	\int_{\R^2} m((w, b); x) \dd \mu((w, b)) = \sum_{i=1}^{m} \max(0, w_i x_j + b_i) \alpha_i p_i
\end{equation}

In this case, the first layer is fixed: the change of direction($\frac{-b_i}{w_i}$) and slope $(w_i)$ of a neuron is described by a mass displacement from point A to point B. 

The movemement is described by a PDE and simulated on a grid. Each point $i$ of the grid has a weight $p_i \in \R$, and as a whole $p \in \R^m$ is the discretized distribution.

The same wasserstein gradient flow can be computed by this step:

\begin{equation}
	\mu(t+1) = \argmin_{\mu \in \mathcal{M}(\paramS)} F(\mu) + \frac{1}{2 \step} W_2(\mu; \mu(t))
\end{equation}

We tried different ways of computing the Wasserstein Gradient Flow. 

\begin{itemize}
	\item JKO stepping: entropic approximation on a fixed grid. Pros: not very dependant on dimension $d$. Cons: add another loop and more parameters to fine tune, introduce diffusion.
		\item Sliced Wasserstein: Pros: midly dependant on $d$ without diffusion. Differentiable with pytorch. Cons: Parameters to tune, distance to true WS distance has to be studied
		\item Direct EMD distance from POT library. Pros: differentiable with pytorch. Cons: Might be slow with $d$
\end{itemize}

Preliminary results using the EMD distance indicate no particular differences between the gradient flow and the wasserstein gradient flow.

\subsection{JKO stepping with Dykstra's algorithm}

\begin{align}
p_{t+1} := & \prox^{W_{\gamma}}_{\tau f}(p_t) \\
							= & \argmin_{p \in \text{simplex}} W_{\gamma}(p, q) + \tau f(p) \\
							= &\argmin_{p \in \text{simplex}}  \left( \min_{\pi \in \Pi(p, q)} \inp{c}{\pi} + \gamma E(\pi) \right) + \tau f(p)
\end{align}

Where $\pi$ is a mapping, $c$ the ground cost for every point on the grid. When the ground cost between two points in the euclidian space is $c_{i,j} = \norm{x_i- x_j}^2$, (and $\gamma=0$, $f$ smooth...), this scheme formaly discretize the above mentionned PDE.

To do the step above, we'll use a bregman splitting approach that replace the single implicit $W_\gamma$ proximal step by many iterative KL implicit proximal steps. Specifically(?) Dykstra's algorithm for JKO stepping. This involve using the gibbs kernel:$\xi = e^{-\frac{c}{\gamma}} \in \mathbb{R}^{N \times N}_{+, \ast}$


\begin{algorithm}
\caption{JKOstep}
\begin{algorithmic}[1]
\State $p \gets p_0 \in \R^m$
\State $q_{\text{norm}} \gets \lVert p \rVert^2$
\State $a, b \gets \mathbf{1}, \mathbf{1} \in \R^m$ \Comment{Initialize vectors with ones}
\For{$i \gets 1$ \textbf{to} $T$}
\State $p \gets \text{prox}^{\text{KL}}_{\tau/\gamma}(\xi b)$
    \State $a \gets p / (\xi b)$
    \State $\text{ConstrEven} \gets \frac{\lVert b \cdot (\xi a) - q \rVert}{q_{\text{norm}}}$
    \State $b \gets q / (\xi a)$
    \State $\text{ConstrOdd} \gets \frac{\lVert a \cdot (\xi b) - p \rVert}{q_{\text{norm}}}$
    
    \If{$\text{ConstrOdd} < \text{tol}$ \textbf{and} $\text{ConstrEven} < \text{tol}$}
        \State \textbf{break}
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

%\includegraphics[width=0.7\textwidth]{bigscale_start}
\section{Classic setup}

Data $x_j \in \R^d$ and labels $y_j \in \R$, $j=1,..,n$

First layer $w_i \in \R^d$, second layer $\alpha_i \in \R$, $i=1,..,m$

$\step>0$ step-size, $\reg$ regularization

\begin{equation}
	 \mathcal{L}(W, \alpha) = \sum_{j=1}^n \bigg( \underbrace{\sum_{i=1}^m \max(0, w_i^\top x_j) \alpha_i}_{\text{Network's Output}} - y_j \bigg)^2 + \underbrace{\lambda \sum_{i=1}^m \| w_i \|^2_2 + \alpha_i^2}_{\text{Weight Decay}}
\end{equation}


\textbf{Discret time.}

Full-batch gradient descent

\begin{equation}
	(W, \alpha)_{t+1} = (W, \alpha)_t - \step \nabla \mathcal{L}((W, \alpha)_t)
\end{equation}

Implicit

\begin{equation}
	\param_{t+1} = \argmin_{\param} \mathcal{L}(\param) +\frac{1}{2\step} \norm{\param - \param_t}
\end{equation}

\textbf{Continuous time.}

Taking $\step \to 0$, we get the gradient flow: $\frac{\dd \param_t}{\dd t} = - \nabla \mathcal{L}(\param_t)$. We make ReLU differentiable with $\sigma'(0)=0$ as justified in \citep{boursierGradientFlowDynamics2022}.

\section{Infinite width, using a measure: mean-field}

\textbf{Mean-field limit}\citep{chizatGlobalConvergenceGradient2018}: For a sufficiently large width, the training dynamics of a NN can be coupled with the evolution of a probability distribution described by a PDE.

If [...] converges, with $m \rightarrow \infty$ (many-particle limit), our particles of interest converges to a Wasserstein gradient flow of F:

\begin{equation}
	\partial \mu_t = - \diver(v_t \mu_t) \text{ where } v_t \in - \partial F'(\mu_t)
\end{equation}

\begin{equation}
	\int_\paramS m(\param; x) \dd \mu(\param) = \frac{1}{m} \sum_{i=1}^{m} \inp{\vw_i}{\vx_j}_+ \alpha_i
\end{equation}

Different ways to use a measure to represent the neurons of a two layer network:

\begin{itemize}
	\item $\paramS = \R^d \times \R$, measure $\mu = \frac{1}{m} \sum_{i=1}^{m} \dirac_{\param_i = (\vw_i, \alpha_i)}$, output of one neuron $m(\param=(\vw,\alpha); \vx) = \inp{\vx}{\vw}_+ \alpha$: (works, output matches discrete)
	\item $\paramS = \R^d$, measure $\mu = \frac{1}{m} \sum_{i=1}^{m} \alpha_i \dirac_{\param_i = \vw_i}$ output of one neuron $m(\param=\vw; \vx) = \inp{\vx}{\vw}_+$ (works)
	\item $\paramS = \R^d \times \R^d$, output of one neuron $m(\tilde{\vw}_+, \tilde{\vw}_-, \vx) = \inp{\tilde{\vw}_+}{\vx} - \inp{\tilde{\vw}_-}{\vx}$ (works, separate neg and positive)
	\item $\paramS = (S^{d-1} \times \R)$, output of one neuron $m((\vd, \tilde{\alpha}); \vx) = \tilde{\alpha} \inp{\vd}{\vx} = \tilde{\alpha}\  \mathds{1}_{\inp{\vd}{\vx}>0} $ (works), mapping: $\vd = \frac{\vw}{\norm{\vw}}$ and $\tilde{\alpha}=\norm{\vw} \alpha$. Gradient are not equal to discrete.
\end{itemize}

\subsection{Algorithm, discretize the measure's space}

Take a grid of $N$ points in $\paramS$, we can match the notation above by taking a neuron for each point of the grid $m=N$.

\begin{equation}
	\mu(t+1) = \argmin_{\mu \in \mathcal{M}(\paramS)} F(\mu) + \frac{1}{2 \step} W_2(\mu; \mu(t))
\end{equation}

% A essayer: KL à la place de distance wasserstein.

% Remark: le 1/m c'est principalement pour être ok à l'infini. Dans le papier JKO \citep{carlierConvergenceEntropicSchemes2017} ils utilisent un vecteur de proba

% Take $\step \to 0$, get gradient flow. Take $m \to \infty$, get wasserstein gradient flow \citep{bachGradientDescentInfinitely2021}, and if it converges, it goes to the global optimal.

\subsection{JKO}

What we compute by using the entropic JKO flow iterations.

\begin{align}
	\forall t > 0, p_{t+1} := & \prox^{W_{\gamma}}_{\tau f}(p_t) \\
							= & \argmin_{p \in \text{simplex}} W_{\gamma}(p, q) + \tau f(p) \\
							= &\argmin_{p \in \text{simplex}}  \left( \min_{\pi \in \Pi(p, q)} \inp{c}{\pi} + \gamma E(\pi) \right) + \tau f(p)
\end{align}

% $f$ "should" be convex and with a closed form proximal % from JKO paper.

\begin{itemize}
	\item \href{https://arxiv.org/pdf/2206.05262.pdf}{Meta Optimal Transport (paper)} and \href{https://github.com/facebookresearch/meta-ot}{(code git)}: InputConvexNN to predict solution of OT problem
	\item \href{https://arxiv.org/pdf/2106.06345.pdf}{JKOnet (paper)} and \href{https://github.com/bunnech/jkonet}{(code git)}:
		\begin{itemize}
			\item \href{https://github.com/bunnech/jkonet/tree/main/jkonet/models}{/models} \verb|->| sinkhorn loss defined in loss.py, differentiable loop in fixed point.py
			\item next step: trying to create the right \href{https://ott-jax.readthedocs.io/en/latest/_autosummary/ott.geometry.geometry.Geometry.html#ott.geometry.geometry.Geometry}{Geometry} object from OTT library, which is what's used for sinkhorn
		\end{itemize}
\end{itemize}

\subsection{Papers}

\href{https://arxiv.org/pdf/1502.06216.pdf}{The algo we try to implement}

Paper with a \href{https://arxiv.org/pdf/1512.02783.pdf}{specific case that doesn't match ours:} 

In the future, \href{https://arxiv.org/pdf/2106.00736.pdf}{large-scale waserstein gradient flows}

\subsubsection{Grid problems}

The grid currently dictate the neuron's scale, giving multiple choices. One solution: duplicate each neuron, make one with a small scale and one with a very big scale.
